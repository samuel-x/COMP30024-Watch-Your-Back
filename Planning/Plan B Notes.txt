Machine Learning
	Reinforcement learning
		Q-Learning
			Section 13.3 (pg 373) in Machine Learning by Tom M. Mitchell (ML TMM.)

	Temporal Difference Learning
		Section 13.5 (pg 383) in ML TMM.
		Seems to be too much like MCTS and so has the same issues.

	Libraries
		Shogun
		TensorFlow
		Sci-kit Learn

Non-machine learning
	Alpha-beta Pruning
		Section 5.3 (pg 167) in AI book.
		
	SSS*
		https://en.wikipedia.org/wiki/SSS*
	Principal Variation Search aka NegaScout
		https://en.wikipedia.org/wiki/Principal_variation_search
		Apparently performs better than both Alpha-beta and SSS*, at the expense of more space.

1 - Issue:
	White sees that it will lose (2, 1) no matter what, so has to do a -1 move. This move is random, but, without white realizing it, is putting another piece into harm's way, resulting in a forced -2 move on their next turn.
		Solution: Heuristic that discourages putting pieces into threatened positions e.g. X W - B (if playing white)?

2 - Bug:
	Edge case.
	Hard to put into words, screenshot helps.
		White doesn't realize that (6, 2) was destroyed.
	Occurs when getting the squares that are going to be killed by the corners changing and the killing piece is part of the "simultaneous move" e.g. X B W where X is a new corner and W was just moved to its position. (B is killed). 
	Reason: getting corner kills considers state of board before the move W is made.
		Probably means that it also will consider B killed in - B W situation where W is moved and - gets replaced with X.
	UPDATE: Appears solved. Will require more testing.

Genetic Algorithm
	https://www.youtube.com/watch?v=RxTfc4JLYKs
	1. Generate population of size N
		- In our case, generate N collections of random weights for our heuristic.

	2. Calculate fitness for N elements
		Need fitness function
			Could be:
			define num_games
			Each AI plays floor(num_games / (n* (n-1) / 2)) games vs every other AI.
				e.g. let's say N = 12 and num_games = 200
				Each AI plays floor(200 / (12 * (12 - 1) / 2)) = floor(3.03) = 3 games vs every other AI.
			Fitness function would be a given AI's win rate.
			Would probably only have time to play with depth = 1. Results should mostly be applicable though, anyway.
			Highly parallelizable though!
		- Apply fitness function to each AI

	3. Reproduction / Selection
		Pick parents
			Pick top 3 as parents (or top 2?)
			Assign each AI a % probability based off of fitness score. e.g. scores are [5, 1, 0 , 4] for N = 4 -> [50%, 10%, 0%, 40%]
				e.g. [0.6, 0.6, 0.2, 0] -> [score / score_sum for score in [0.6, 0.6, 0.2, 0]] -> ~[0.429, 0.429, 0.143, 0.0] - sum > 1.0
		Make new AI
			Crossover
				Semi-randomly inherit components of parents to mix together into a new AI.
					For us, perhaps average all weights?
					Alternative (see https://stackoverflow.com/a/45202429/5372500)
						E.g. weight 42.
						42 -> 101010
						Do stuff
						Not easy since we want negative weights as well.
			Mutation
				e.g. mutation_rate = 0.02 i.e. 2%
				Take cross-over'd child value and multiply by random.uniform(0.98, 1.02)
					Might lead to local minima. Consider the 42 -> 101010 stuff where each bit has a % chance of flipping.
		Add new AI to new population.
	4. Repeat step 2 onwards indefinitely.

TODO:
	1. Add more heuristics
		- Cohesiveness
			- Average distance to allied pieces
			-  + WHITE_COHESIVENESS_WEIGHT * white_cohesiveness - BLACK_COHESIVENESS_WEIGHT * black_cohesiveness
		- Mobility
			- Number of available moves
			- + WHITE_MOBILITY_WEIGHT * white_mobility - BLACK_MOBILITY_WEIGHT * black_mobility
		- Center 4 spots
			- Number of center spots held / 4
			- + WHITE_CENTER_CONTROL_WEIGHT * white_center_control - BLACK_CENTER_CONTROL_WEIGHT * black_center_control

	2. Implement genetic algorithm
		- See "Genetic Algorithm" instructions above.
		- Adapt referee.py to facilitate.
		- Use threads e.g. every game is played on its own thread.
		- Have a PlayerWrapper class to contain weights.
		- Likely need to temporarly change Player class to allow for setting the weights, rather than them being hardcoded as they are now.

	3. Comment.txt
		- Talk about MCTS.
		- Talk about Alpha-beta pruning.
		- Talk about attempt (hopefully successful :P) at utilizing genetic algorithms for finding the best heuristic weights.

	?. Depth depends on the time remaining.
		- Each round can spend REMAINING_TIME / num_expected_rounds
		- How to estimate num_expected rounds?
			- Can hardcode at 200?
		- Once out of time, choose move with best alpha-beta value at that moment.
			- Or just decide "I don't have much time this round, I'll just 1 depth" or visa versa i.e. "I have lots of time this round, I'll use 2 depth"
				- Base time thresholds off of observed max-expected time to make move e.g. "Depth 1 decisions take max 0.2 seconds" or "Depth 2 decisions take max 5 seconds"

	?. Optimization
		- Anything we can do to speed up the existing framework?
		- Copying the board when using board.get_next_board() takes a lot of time.
			- Unfortunately we're using Python so I don't know how much control we have over data types, but maybe it's possible to use smaller data types e.g. Pos2D only needs to contain values of 0 - 7.